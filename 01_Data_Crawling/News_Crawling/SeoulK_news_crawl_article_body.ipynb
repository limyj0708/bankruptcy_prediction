{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "# from IPython.display import clear_output # Cell output clear를 위함\n",
    "import socket\n",
    "hostname = socket.gethostname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_company_list = pd.read_csv('Target_company_list_for_news_crawling.csv', encoding='utf-8', dtype={'종목코드': object, '상장폐지일': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_create_page_content(url, company_name, stock_code):\n",
    "    headers = {'user-agent': 'Mozilla/5.0'}\n",
    "    req = requests.get(url, headers=headers)\n",
    "    html = req.text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    dl_list = soup.find_all('dl')\n",
    "    \n",
    "    if len(dl_list) == 0:\n",
    "        # 페이지에 기사가 하나도 없으면 None 반환\n",
    "        return None\n",
    "    contents_of_one_page = []\n",
    "    \n",
    "    for each in dl_list:\n",
    "        contents_of_each_article = {'news_comp':'서울경제',\\\n",
    "                                    'target_comp':company_name, 'target_code':stock_code,\\\n",
    "                                    'article_link':None, 'published_date':None, 'title':None}\n",
    "        \n",
    "        title = each.find('dt').find('a').get_text()\n",
    "        article_link = 'https://www.sedaily.com' + each.find('dt').find('a').get('href')\n",
    "        published_date = each.find('dd').find('span', class_='letter').get_text()\n",
    "        \n",
    "        contents_of_each_article['title'] = title\n",
    "        contents_of_each_article['article_link'] = article_link\n",
    "        contents_of_each_article['published_date'] = published_date\n",
    "        \n",
    "        contents_of_one_page.append(contents_of_each_article)\n",
    "    return contents_of_one_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_six_month_daterange_list(date='20191231'):\n",
    "    delisted_date_datetime_obj = datetime.datetime.strptime(date, '%Y%m%d')\n",
    "    start_date = (delisted_date_datetime_obj + datetime.timedelta(days=-1095)).strftime(\"%Y-%m-%d\")\n",
    "    end_date = delisted_date_datetime_obj.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    date_list = []\n",
    "    for idx in range(1,7):\n",
    "        end_date = (delisted_date_datetime_obj + datetime.timedelta(days=(-183*(idx-1)))).strftime(\"%Y-%m-%d\")\n",
    "        start_date = (delisted_date_datetime_obj + datetime.timedelta(days=(-183*idx+1))).strftime(\"%Y-%m-%d\")\n",
    "        date_list.append((start_date, end_date))\n",
    "    \n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv_by_every_page(list_param, header_list):\n",
    "    if os.path.isfile(hostname + '_seoulK.csv'): # 파일이 있는지 확인\n",
    "        open_param = 'a' # 있으면 수정(뒤에 내용 추가) 모드\n",
    "    else:\n",
    "        open_param = 'w' # 없으면 파일 생성함\n",
    "            \n",
    "    with open(hostname + '_seoulK.csv', open_param, encoding='utf-8', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        \n",
    "        if open_param == 'w':\n",
    "            wr.writerow(header_list)\n",
    "            \n",
    "        for each_dic in list_param:\n",
    "            writerow_pa_list = []\n",
    "            for each_key in each_dic:\n",
    "                writerow_pa_list.append(each_dic[each_key])\n",
    "            wr.writerow(writerow_pa_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seoulk_article_url(df_company_list):\n",
    "    try: # pickle 파일 있는지 없는지 확인\n",
    "        with open('progress.pickle', 'rb') as f:\n",
    "            progress_dict = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        progress_dict = {}\n",
    "    \n",
    "    for idx in df_company_list.index:\n",
    "        stock_code = df_company_list.iloc[idx]['종목코드']\n",
    "        if stock_code not in progress_dict.keys():\n",
    "            # 진행도 저장 dictionary에 지금 하는 종목코드가 있는지 확인\n",
    "            progress_dict[stock_code] = [0,0,0]\n",
    "            # 없으면 종목코드 항목 생성하고 완성 진행도 0, 날짜 진행도 0, 페이지 진행도 0으로 세팅\n",
    "        elif progress_dict[stock_code][0] == 1:\n",
    "            #이 종목이 크롤링 완성된 종목이라면, 다음 종목으로 넘어감\n",
    "            continue\n",
    "\n",
    "        company_name = df_company_list.iloc[idx]['기업명']\n",
    "        delisted_check = df_company_list.iloc[idx]['상폐여부']\n",
    "        delisted_date = df_company_list.iloc[idx]['상장폐지일']\n",
    "        header = ['언론사명', '회사명', '종목코드', '기사_URL', '기사_업로드_날짜', '기사_제목']\n",
    "\n",
    "        skip_num_date = progress_dict[stock_code][1] # 이만큼 날짜를 넘길 예정\n",
    "        \n",
    "        if delisted_check == 1: # 상폐 종목인지 확인\n",
    "            date_list = create_six_month_daterange_list(delisted_date)\n",
    "        else:\n",
    "            date_list = create_six_month_daterange_list('20191231')\n",
    "\n",
    "        for idx, each_pair in enumerate(date_list):\n",
    "            # clear_output(wait=True)\n",
    "            os.system('cls' if os.name == 'nt' else 'clear')\n",
    "            page_num = 1\n",
    "            if skip_num_date >= (idx+1): # 지난 날짜 진행도가 3이면, (idx+1)이 4가 될 때까지 반복문 공회전\n",
    "                continue\n",
    "            skip_num_page = progress_dict[stock_code][2] # 이만큼 페이지를 넘길 예정\n",
    "            while True: # 1페이지씩 탐색함\n",
    "                if skip_num_page >= page_num: # 지난 진행도가 10이면, pagenum이 11이 될 때까지 반복문 공회전\n",
    "                    page_num += 1 \n",
    "                    continue\n",
    "                \n",
    "                url='https://www.sedaily.com/Search/Search/SEList?Page=' + str(page_num) + \\\n",
    "                                     '&scDetail=detail&scOrdBy=0&catView=AL&scText='+ company_name + \\\n",
    "                                     '&scPeriod=0&scArea=tc&scTextIn=&scTextExt=&scPeriodS=' + each_pair[0] + \\\n",
    "                                     '&scPeriodE=' + each_pair[1]\n",
    "                                     # date는 YYYY-MM-DD\n",
    "\n",
    "                article_list_of_this_page = get_and_create_page_content(url, company_name, stock_code)\n",
    "                if article_list_of_this_page == None:\n",
    "                    break # 이 페이지부터는 기사가 없음. 즉 검색 끝난 것. 다음 연도쌍으로 넘어감\n",
    "                \n",
    "                print(f'{company_name}, {page_num}, {each_pair[0]}, {each_pair[1]}')\n",
    "                save_to_csv_by_every_page(article_list_of_this_page, header) # 기사 저장\n",
    "                page_num += 1\n",
    "                \n",
    "                progress_dict[stock_code][2] = progress_dict[stock_code][2] + 1 ## 페이지 진행도 1 더함\n",
    "                with open('progress.pickle', 'wb') as f:\n",
    "                    pickle.dump(progress_dict, f)\n",
    "            \n",
    "            progress_dict[stock_code][1] = progress_dict[stock_code][1] + 1 ## 날짜 진행도 1 더함\n",
    "            progress_dict[stock_code][2] = 0 ## 날짜가 다음으로 넘어갔으니, 페이지 진행도는 다시 0이 되어야 함\n",
    "            with open('progress.pickle', 'wb') as f:\n",
    "                pickle.dump(progress_dict, f)\n",
    "        \n",
    "        progress_dict[stock_code][0] = 1 # 이 종목은 완료되었음\n",
    "        with open('progress.pickle', 'wb') as f:\n",
    "            pickle.dump(progress_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_seoulk_article_url(df_company_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
