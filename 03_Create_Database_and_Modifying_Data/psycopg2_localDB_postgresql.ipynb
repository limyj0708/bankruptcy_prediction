{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import types\n",
    "import pandas.io.sql as sqlio\n",
    "from psycopg2 import sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../db_config.json\", \"r\") as db_config:\n",
    "    db_config = json.load(db_config)\n",
    "\n",
    "# db 접속정보는 json으로 관리\n",
    "dbname = db_config['local_db']['NAME']\n",
    "host = db_config['local_db']['HOST'] \n",
    "port = db_config['local_db']['PORT'] \n",
    "user = db_config['local_db']['USER']\n",
    "password= db_config['local_db']['PASSWORD'] \n",
    "connect_param_local = dict({'dbname':dbname, 'host':host, 'port':port, 'user':user, 'password':password})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 대상 회사 테이블 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/youngjinlim/OneDrive/Coding/BigData_Study/Final_project/Data/Company_list/company_list_df.pkl', 'rb') as f:\n",
    "    company_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대상 회사 테이블 만들기\n",
    "with psycopg2.connect(**connect_param_local) as con:\n",
    "    with con.cursor() as cur:\n",
    "        cur.execute('drop table if exists target_company_list')\n",
    "        create_companylist_sql = '''\n",
    "        create table target_company_list(\n",
    "        stock_code char(6) primary key\n",
    "        ,company_name varchar(50) NOT NULL\n",
    "        ,market char(1) NOT NULL\n",
    "        ,bankruptcy char(1) NOT NULL\n",
    "        ,delisted_check char(1) NOT NULL\n",
    "        ,delisted_date date NOT NULL\n",
    "        ,pre_3m date NOT NULL\n",
    "        ,pre_6m date NOT NULL\n",
    "        ,pre_1y date NOT NULL\n",
    "        );\n",
    "        '''\n",
    "        cur.execute(create_companylist_sql)\n",
    "        con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 대상 회사 테이블에 값 넣기\n",
    "with psycopg2.connect(**connect_param_local) as con:\n",
    "    with con.cursor() as cur:\n",
    "        insert_companyvalue_sql = 'insert into target_company_list values (%s, %s, %s, %s, %s, %s, %s, %s, %s);'\n",
    "        for each_idx in company_list.index:\n",
    "            tuple_inserted_listtype = []\n",
    "            for each_value in company_list.iloc[each_idx].values:\n",
    "                if str(type(each_value)) == \"<class 'numpy.int64'>\":\n",
    "                    tuple_inserted_listtype.append(int(each_value))\n",
    "                else:\n",
    "                    tuple_inserted_listtype.append(each_value)\n",
    "            cur.execute(insert_companyvalue_sql, tuple(tuple_inserted_listtype))\n",
    "            con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 주식 데이터 테이블 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. 주식 데이터 테이블 원본 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_company_list에 없는 종목 제거. 딱 하나였음\n",
    "df_cache = pd.read_csv('stock_data_raw_transposed.csv', chunksize=100000, dtype='object')\n",
    "head_check = 1\n",
    "for each_chunk in df_cache:\n",
    "    df_cache2 = each_chunk[each_chunk['stock_code'] != '042950']\n",
    "    if head_check == 1:\n",
    "        df_cache2.to_csv('stock_data_raw_transposed.csv', mode='a', index=False)\n",
    "        head_check += 1\n",
    "    else:\n",
    "        df_cache2.to_csv('stock_data_raw_transposed.csv', mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주식 데이터 테이블 만들기\n",
    "get_header = next(pd.read_csv('stock_data_raw.csv', chunksize=1)).columns\n",
    "with psycopg2.connect(**connect_param_local) as con:\n",
    "    with con.cursor() as cur:\n",
    "        cur.execute('drop table if exists stock_data_2000_2020_raw')\n",
    "        \n",
    "        create_stocktable_sql = '''\n",
    "        create table stock_data_2000_2020_raw(\n",
    "        company_name varchar(50) NOT NULL\n",
    "        ,date date NOT NULL\n",
    "        ,stock_code char(6) NOT NULL\n",
    "        ,FOREIGN KEY (stock_code) REFERENCES target_company_list(stock_code));\n",
    "        '''\n",
    "        cur.execute(create_stocktable_sql)\n",
    "        \n",
    "        prefix_add_columns = 'ALTER TABLE stock_data_2000_2020_raw ADD COLUMN'\n",
    "        for each_column in get_header[3:]:\n",
    "            add_columns_to_table = prefix_add_columns + ' ' + '\"' + each_column + '\"' + ' double precision'\n",
    "            cur.execute(add_columns_to_table)\n",
    "        \n",
    "        con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv copy. 내용 채워넣기\n",
    "with psycopg2.connect(**connect_param_local) as con:\n",
    "    with con.cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            COPY stock_data_2000_2020_raw\n",
    "            from '/usr/local/var/postgres/stock_data_raw.csv'\n",
    "            DELIMITER ','\n",
    "            CSV HEADER;\n",
    "            \"\"\"\n",
    "        )\n",
    "        con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. 주가 데이터 3년치만 담고 있는 테이블 만들기\n",
    "* 3개월 전 예측\n",
    "* 6개월 전 예측\n",
    "* 1년 전 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_finance_table_for_ml(prediction_range:'6m', years:3, timedelta:1096):\n",
    "    get_header = next(pd.read_csv('csv_data/stock_data_raw.csv', chunksize=1)).columns\n",
    "    \n",
    "    with psycopg2.connect(**connect_param_local) as con:\n",
    "        with con.cursor() as cur:\n",
    "            table_name_argu = 'stock_data_' + str(years) + 'years_raw_' + prediction_range\n",
    "            drop_table_query = sql.SQL('drop table if exists {table_name}').format(table_name=sql.Identifier(table_name_argu))\n",
    "            cur.execute(drop_table_query)\n",
    "            \n",
    "            create_stocktable_sql = \"\"\"\n",
    "            create table {table_name} (\n",
    "            ,company_name varchar(50) NOT NULL\n",
    "            ,date date NOT NULL\n",
    "            ,stock_code char(6) NOT NULL\n",
    "            ,FOREIGN KEY (stock_code) REFERENCES target_company_list(stock_code));\n",
    "            \"\"\".format(table_name = sql.Identifier(table_name = sql.Identifier(table_name_argu))\n",
    "            cur.execute(create_stocktable_sql)\n",
    "            \n",
    "            con.commit()\n",
    "\n",
    "            # 테이블에 열 추가하기\n",
    "            prefix_add_columns = sql.SQL('ALTER TABLE {table_name} ADD COLUMN').format(table_name = sql.Identifier(table_name = sql.Identifier(table_name_argu))\n",
    "            for each_column in get_header[3:]:\n",
    "                add_columns_to_table = prefix_add_columns + ' ' + '\"' + each_column + '\"' + ' double precision'\n",
    "                print(add_columns_to_table)\n",
    "                cur.execute(add_columns_to_table)\n",
    "            con.commit()\n",
    "                                                                                       \n",
    "                                                                                       \n",
    "    with psycopg2.connect(**connect_param_local) as con:\n",
    "        cur_2 = con.cursor()\n",
    "        # fetchall을 사용해서 커서를 재활용하지 않고 커서를 두 개 두는 이유는, redshift의 single-node cluster에서는 fetchall이 지원되지 않기 떄문이다.\n",
    "            # InternalError_: Fetch ALL is not supported on single-node clusters.\n",
    "            # Please specify the fetch size (maximum 1000 for single-node clusters) \n",
    "            # or upgrade to a multi node installation.\n",
    "\n",
    "        get_companylist_sql = \"\"\"select * from target_company_list;\"\"\"\n",
    "        target_company_list = sqlio.read_sql_query(get_companylist_sql, con)\n",
    "        # 1mb도 안되는 작은 테이블이라서 데이터프레임으로 한 번에 받아옴\n",
    "\n",
    "        for each_code in target_company_list['stock_code']:\n",
    "            cur_1 = con.cursor('ss_cursor') # server side cursor\n",
    "            # cur_1.itersize = 1000 # redshift single-node cluster에서의 server side cursor의 최대 제한값\n",
    "\n",
    "            print(each_code,'_start')\n",
    "            predict_start = target_company_list[target_company_list['stock_code'] == each_code]['pre_' + prediction_range].values[0] - datetime.timedelta(days=1)\n",
    "            predict_end = predict_start - datetime.timedelta(days = timedelta)\n",
    "\n",
    "            cur_1.execute(\n",
    "                \"\"\"\n",
    "                select * from stock_data_2000_2020_raw\n",
    "                where (date between %(predict_end)s and %(predict_start)s) and (stock_code = %(stock_code)s);\n",
    "                \"\"\",\n",
    "                {'predict_end':predict_end.strftime(\"%Y-%m-%d\"),'predict_start':predict_start.strftime(\"%Y-%m-%d\"),'stock_code': each_code}\n",
    "            )\n",
    "\n",
    "            for each_row in cur_1:\n",
    "                # fetchall을 못 써서 cur_2가 등장하는 부분\n",
    "                # next(cur_1)로 한줄씩 불러와서 insert\n",
    "                query_insert = sql.SQL('insert into {table_name} values %s').format(table_name = sql.Identifier(table_name_argu))\n",
    "                cur_2.execute(query_insert, [each_row]) # each_row는 tuple이다.\n",
    "\n",
    "            con.commit()\n",
    "            print(each_code,'_commit complete')\n",
    "        cur_2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 감사보고서 데이터 테이블 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg2.connect(**connect_param_local) as con:\n",
    "    with con.cursor() as cur:\n",
    "        # 감사보고서 테이블 만들기\n",
    "        cur.execute('drop table if exists dart_audit_report_data')\n",
    "        create_stocktable_sql = \"\"\"\n",
    "        create table dart_audit_report_data(\n",
    "        stock_code char(6)\n",
    "        ,FOREIGN KEY (stock_code) REFERENCES target_company_list(stock_code)\n",
    "        ,company_name varchar(50) NOT NULL\n",
    "        ,report_name char(4)\n",
    "        ,n_opinion boolean\n",
    "        ,report_date date);\n",
    "        \"\"\"\n",
    "        cur.execute(create_stocktable_sql)\n",
    "        con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv copy\n",
    "with psycopg2.connect(**connect_param_local) as con:\n",
    "    with con.cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            COPY dart_audit_report_data\n",
    "            from '/usr/local/var/postgres/dart_audit_report_data_refined.csv'\n",
    "            DELIMITER ','\n",
    "            CSV HEADER;\n",
    "            \"\"\"\n",
    "        )\n",
    "        con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 재무 데이터 테이블 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1. 재무 데이터 전체 원본 테이블 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총자산(천원)\n",
      "유동자산(천원)\n",
      "현금및현금성자산(천원)\n",
      "재고자산(천원)\n",
      "비유동자산(천원)\n",
      "총부채(천원)\n",
      "유동부채(천원)\n",
      "비유동부채(천원)\n",
      "총자본(천원)\n",
      "매출액(천원)\n",
      "당기순이익(천원)\n",
      "세전계속사업이익(천원)\n",
      "영업이익(천원)\n",
      "이자비용(천원)\n",
      "비지배주주지분(천원)\n",
      "계속사업법인세비용(천원)\n",
      "매출총이익(천원)\n",
      "금융원가(비영업)(천원)\n",
      "부채비율(p)\n",
      "자기자본비율(p)\n",
      "차입금의존도(p)\n",
      "당기순이익률(p)\n",
      "세전계속사업이익률(p)\n",
      "영업이익률(p)\n",
      "매출총이익률(p)\n",
      "ROE(당기순이익)(p)\n",
      "총자산회전율(회)\n",
      "매출액증가율(전년동기)(p)\n",
      "총자산증가율(전년동기)(p)\n",
      "당기순이익증가율(전년동기)(p)\n",
      "EPS증가율(전년동기)(p)\n"
     ]
    }
   ],
   "source": [
    "df_cache = pd.read_csv('/Users/youngjinlim/OneDrive/Coding/BigData_Study/Final_project/AWS/COPY용 데이터/finance_data_1999_2020_raw.csv', dtype={'stock_code':object}, chunksize=1)\n",
    "get_header_finance = next(df_cache).columns\n",
    "\n",
    "with psycopg2.connect(**connect_param_local) as con:\n",
    "    with con.cursor() as cur:\n",
    "        # 재무 데이터 테이블 만들기\n",
    "        cur.execute('drop table if exists finance_data_1999_2020_raw')\n",
    "        create_stocktable_sql = '''\n",
    "        create table finance_data_1999_2020_raw(\n",
    "        stock_code char(6) NOT NULL\n",
    "        ,FOREIGN KEY (stock_code) REFERENCES target_company_list(stock_code)\n",
    "        ,company_name varchar(50) NOT NULL\n",
    "        ,결산월 smallint\n",
    "        ,회계년 smallint\n",
    "        );\n",
    "        '''\n",
    "        cur.execute(create_stocktable_sql)\n",
    "        con.commit()\n",
    "        \n",
    "        # 테이블에 열 추가하기\n",
    "        prefix_add_columns = 'ALTER TABLE finance_data_1999_2020_raw ADD COLUMN'\n",
    "        for each_column in get_header_finance[4:]:\n",
    "            if each_column in ['부채비율(p)', '자기자본비율(p)' ,'당기순이익증가율(전년동기)(p)', 'EPS증가율(전년동기)(p)', 'ROE(당기순이익)(p)', '매출액증가율(전년동기)(p)']:\n",
    "                add_columns_to_table = prefix_add_columns + ' ' + '\"' + each_column + '\"' + ' varchar(15)' \n",
    "                    # 계산불가능한 항목(divided by 0 등)은 별도의 텍스트로 표시되어있기 때문에 varchar 선택 \n",
    "            else:\n",
    "                add_columns_to_table = prefix_add_columns + ' ' + '\"' + each_column + '\"' + ' double precision'\n",
    "            cur.execute(add_columns_to_table)\n",
    "            print(each_column)\n",
    "        con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv copy\n",
    "with psycopg2.connect(**connect_param_local) as con:\n",
    "    with con.cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            COPY finance_data_1999_2020_raw\n",
    "            from '/usr/local/var/postgres/finance_data_1999_2020_raw.csv'\n",
    "            DELIMITER ','\n",
    "            CSV HEADER;\n",
    "            \"\"\"\n",
    "        )\n",
    "        con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼 두 개 추가\n",
    "with psycopg2.connect(**connect_param_local) as con:\n",
    "    with con.cursor() as cur:\n",
    "        cur.execute('ALTER TABLE finance_data_1999_2020_raw ADD COLUMN \"사업보고서마감일\" date')\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            UPDATE\n",
    "            finance_data_1999_2020_raw\n",
    "            SET\n",
    "            \"사업보고서마감일\" = (date_trunc('month',  (\"회계년\" || '-' || \"결산월\" || '-' || 10)::date) + interval '1 month' - interval '1 day')::date+90;\n",
    "            \"\"\"\n",
    "        ) # 각 월의 최종일에서 90일을 더한 값을 저장하는 열을 추가한다. month 단위로 계산되기 때문에, 결산월 뒤의 일은 아무거나 가져다 붙여도 된다. 여기서는 10일을 넣었음\n",
    "        # 결산월 90일 이내에는 사업보고서 공시를 올려야 하기 때문에, 결산월 90일 후에는 무조건 데이터를 볼 수 있을 것이라고 가정한 것이다.\n",
    "        cur.execute('ALTER TABLE finance_data_1999_2020_raw ADD COLUMN \"총자산부채비율(p)\" double precision')\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            UPDATE\n",
    "            finance_data_1999_2020_raw\n",
    "            SET\n",
    "            \"총자산부채비율(p)\" = \"총부채(천원)\"/\"총자산(천원)\";\n",
    "            \"\"\"\n",
    "        )\n",
    "        con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 생성\n",
    "with psycopg2.connect(**connect_param_local) as con:\n",
    "    with con.cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            CREATE INDEX \"idx_(stock_code+사업보고서마감일)_finance_data_1999_2020_raw\"\n",
    "            ON finance_data_1999_2020_raw (stock_code, \"사업보고서마감일\");\n",
    "            \"\"\"\n",
    "        )\n",
    "        con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-2. 재무 데이터 머신러닝 학습용 2년치, 3년치 테이블 만들기\n",
    "* 3개월 후 예측\n",
    "* 6개월 후 예측\n",
    "* 1년 후 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_1_column_finance_3 = [\n",
    "    'Y-1_총자산(천원)'\n",
    "    ,'Y-1_현금및현금성자산(천원)'\n",
    "    ,'Y-1_총부채(천원)'\n",
    "    ,'Y-1_총자본(천원)'\n",
    "    ,'Y-1_매출액(천원)'\n",
    "    ,'Y-1_당기순이익(천원)'\n",
    "    ,'Y-1_세전계속사업이익(천원)'\n",
    "    ,'Y-1_영업이익(천원)'\n",
    "    ,'Y-1_매출총이익(천원)'\n",
    "    ,'Y-1_차입금의존도(p)'\n",
    "    ,'Y-1_당기순이익률(p)'\n",
    "    ,'Y-1_세전계속사업이익률(p)'\n",
    "    ,'Y-1_영업이익률(p)'\n",
    "    ,'Y-1_매출총이익률(p)'\n",
    "    ,'Y-1_총자산회전율(회)'\n",
    "    ,'Y-1_총자산증가율(p)'\n",
    "    ,'Y-1_총자산부채비율(p)'\n",
    "    ,'Y-2_총자산(천원)'\n",
    "    ,'Y-2_현금및현금성자산(천원)'\n",
    "    ,'Y-2_총부채(천원)'\n",
    "    ,'Y-2_총자본(천원)'\n",
    "    ,'Y-2_매출액(천원)'\n",
    "    ,'Y-2_당기순이익(천원)'\n",
    "    ,'Y-2_세전계속사업이익(천원)'\n",
    "    ,'Y-2_영업이익(천원)'\n",
    "    ,'Y-2_매출총이익(천원)'\n",
    "    ,'Y-2_차입금의존도(p)'\n",
    "    ,'Y-2_당기순이익률(p)'\n",
    "    ,'Y-2_세전계속사업이익률(p)'\n",
    "    ,'Y-2_영업이익률(p)'\n",
    "    ,'Y-2_매출총이익률(p)'\n",
    "    ,'Y-2_총자산회전율(회)'\n",
    "    ,'Y-2_총자산증가율(p)'\n",
    "    ,'Y-2_총자산부채비율(p)'\n",
    "    ,'Y-3_총자산(천원)'\n",
    "    ,'Y-3_현금및현금성자산(천원)'\n",
    "    ,'Y-3_총부채(천원)'\n",
    "    ,'Y-3_총자본(천원)'\n",
    "    ,'Y-3_매출액(천원)'\n",
    "    ,'Y-3_당기순이익(천원)'\n",
    "    ,'Y-3_세전계속사업이익(천원)'\n",
    "    ,'Y-3_영업이익(천원)'\n",
    "    ,'Y-3_매출총이익(천원)'\n",
    "    ,'Y-3_차입금의존도(p)'\n",
    "    ,'Y-3_당기순이익률(p)'\n",
    "    ,'Y-3_세전계속사업이익률(p)'\n",
    "    ,'Y-3_영업이익률(p)'\n",
    "    ,'Y-3_매출총이익률(p)'\n",
    "    ,'Y-3_총자산회전율(회)'\n",
    "    ,'Y-3_총자산증가율(p)'\n",
    "    ,'Y-3_총자산부채비율(p)'\n",
    "]\n",
    "\n",
    "step_1_column_finance_2 = [\n",
    "    'Y-1_총자산(천원)'\n",
    "    ,'Y-1_현금및현금성자산(천원)'\n",
    "    ,'Y-1_총부채(천원)'\n",
    "    ,'Y-1_총자본(천원)'\n",
    "    ,'Y-1_매출액(천원)'\n",
    "    ,'Y-1_당기순이익(천원)'\n",
    "    ,'Y-1_세전계속사업이익(천원)'\n",
    "    ,'Y-1_영업이익(천원)'\n",
    "    ,'Y-1_매출총이익(천원)'\n",
    "    ,'Y-1_차입금의존도(p)'\n",
    "    ,'Y-1_당기순이익률(p)'\n",
    "    ,'Y-1_세전계속사업이익률(p)'\n",
    "    ,'Y-1_영업이익률(p)'\n",
    "    ,'Y-1_매출총이익률(p)'\n",
    "    ,'Y-1_총자산회전율(회)'\n",
    "    ,'Y-1_총자산증가율(p)'\n",
    "    ,'Y-1_총자산부채비율(p)'\n",
    "    ,'Y-2_총자산(천원)'\n",
    "    ,'Y-2_현금및현금성자산(천원)'\n",
    "    ,'Y-2_총부채(천원)'\n",
    "    ,'Y-2_총자본(천원)'\n",
    "    ,'Y-2_매출액(천원)'\n",
    "    ,'Y-2_당기순이익(천원)'\n",
    "    ,'Y-2_세전계속사업이익(천원)'\n",
    "    ,'Y-2_영업이익(천원)'\n",
    "    ,'Y-2_매출총이익(천원)'\n",
    "    ,'Y-2_차입금의존도(p)'\n",
    "    ,'Y-2_당기순이익률(p)'\n",
    "    ,'Y-2_세전계속사업이익률(p)'\n",
    "    ,'Y-2_영업이익률(p)'\n",
    "    ,'Y-2_매출총이익률(p)'\n",
    "    ,'Y-2_총자산회전율(회)'\n",
    "    ,'Y-2_총자산증가율(p)'\n",
    "    ,'Y-2_총자산부채비율(p)'\n",
    "]\n",
    "\n",
    "extracted_value_finance= [\n",
    "    '총자산(천원)'\n",
    "    ,'현금및현금성자산(천원)'\n",
    "    ,'총부채(천원)'\n",
    "    ,'총자본(천원)'\n",
    "    ,'매출액(천원)'\n",
    "    ,'당기순이익(천원)'\n",
    "    ,'세전계속사업이익(천원)'\n",
    "    ,'영업이익(천원)'\n",
    "    ,'매출총이익(천원)'\n",
    "    ,'차입금의존도(p)'\n",
    "    ,'당기순이익률(p)'\n",
    "    ,'세전계속사업이익률(p)'\n",
    "    ,'영업이익률(p)'\n",
    "    ,'매출총이익률(p)'\n",
    "    ,'총자산회전율(회)'\n",
    "    ,'총자산증가율(전년동기)(p)'\n",
    "    ,'총자산부채비율(p)'  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_finance_table_for_ml(prediction_range:'6m', years:2, timedelta:730):\n",
    "    \n",
    "    table_name_argument = 'table_for_dt_based_model_' + prediction_range + '_finance_' + str(years) + 'y'\n",
    "    if years == 3:\n",
    "        step_1_column_finance = step_1_column_finance_3\n",
    "    elif years == 2:\n",
    "        step_1_column_finance = step_1_column_finance_2\n",
    "    \n",
    "    with psycopg2.connect(**connect_param_local) as con:\n",
    "        with con.cursor() as cur:\n",
    "            # 재무 데이터 테이블 만들기\n",
    "            query_drop_table = sql.SQL('drop table if exists {table_name}').format(table_name = sql.Identifier(table_name_argument))\n",
    "            cur.execute(query_drop_table)\n",
    "            create_stocktable_sql = sql.SQL('''\n",
    "            create table {table_name}(\n",
    "            stock_code char(6) NOT NULL,\n",
    "            FOREIGN KEY (stock_code) REFERENCES target_company_list(stock_code)\n",
    "            );\n",
    "            ''').format(table_name = sql.Identifier(table_name_argument))\n",
    "            cur.execute(create_stocktable_sql)\n",
    "            con.commit()\n",
    "\n",
    "            # 테이블에 열 추가하기\n",
    "            for each_column in step_1_column_finance:\n",
    "                query_prefix_add_columns = sql.SQL('ALTER TABLE {table_name} ADD COLUMN {column} double precision').format(table_name = sql.Identifier(table_name_argument), column = sql.Identifier(each_column))\n",
    "                cur.execute(query_prefix_add_columns)\n",
    "                print(each_column)\n",
    "            con.commit()\n",
    "\n",
    "\n",
    "    i = 1\n",
    "    insert_values_query_setting = '%s'\n",
    "    while (i <= len(step_1_column_finance)):\n",
    "        insert_values_query_setting = insert_values_query_setting + ',%s'\n",
    "        i+=1\n",
    "    print(insert_values_query_setting)\n",
    "    \n",
    "    \n",
    "    with psycopg2.connect(**connect_param_local) as con:\n",
    "        with con.cursor() as cur:\n",
    "            get_stock_code_query = \"\"\"\n",
    "            SELECT DISTINCT stock_code from finance_data_1999_2020_raw;\n",
    "            \"\"\"\n",
    "\n",
    "            get_date_query = sql.SQL(\n",
    "                \"\"\"\n",
    "                select {pre_date_column} \n",
    "                from target_company_list \n",
    "                where stock_code = %(stock_code)s;\n",
    "                \"\"\"\n",
    "            ).format(pre_date_column = sql.Identifier('pre_' + prediction_range))\n",
    "            \n",
    "            create_cache_query = \"\"\"\n",
    "            create view finance_cache\n",
    "            as (\n",
    "            select *,\n",
    "            row_number() over(order by \"회계년\" desc) as rn\n",
    "            from finance_data_1999_2020_raw\n",
    "            where (stock_code = %(stock_code)s) AND (\"사업보고서마감일\" < %(pre_date)s)\n",
    "            );\n",
    "            \"\"\"\n",
    "\n",
    "            cur.execute(get_stock_code_query)\n",
    "            stock_codes = cur.fetchall()\n",
    "\n",
    "            for each_code in stock_codes:\n",
    "                cur.execute(get_date_query, {'stock_code':each_code[0]})\n",
    "                pre_date_extracted = cur.fetchone()[0].strftime(\"%Y-%m-%d\")\n",
    "                drop_cache_query = \"drop view if exists finance_cache;\"\n",
    "                cur.execute(drop_cache_query)\n",
    "                con.commit()\n",
    "                cur.execute(create_cache_query, {'stock_code':each_code[0], 'pre_date':pre_date_extracted})\n",
    "                con.commit()\n",
    "\n",
    "                insert_list = []\n",
    "                insert_list.append(each_code[0])\n",
    "                for y_num in range(1,years+1): # 최신 year년치만\n",
    "                    for each_value in extracted_value_finance:\n",
    "                        select_target_value_query = sql.SQL(\"\"\"\n",
    "                        select {target_value}\n",
    "                        from finance_cache\n",
    "                        where rn = %(y_num)s;\"\"\").format(target_value = sql.Identifier(each_value))\n",
    "                        cur.execute(select_target_value_query, {'y_num':y_num})\n",
    "                        insert_value = cur.fetchone()\n",
    "                        if insert_value == None:\n",
    "                            insert_list.append(None)\n",
    "                        else:\n",
    "                            # print(insert_value[0])\n",
    "                            insert_list.append(insert_value[0])\n",
    "\n",
    "                insert_query_cache = 'insert into {table_name} values' + '(' + insert_values_query_setting + ')'\n",
    "                insert_query = sql.SQL(insert_query_cache).format(table_name = sql.Identifier(table_name_argument))\n",
    "                cur.execute(insert_query,tuple(insert_list))\n",
    "                con.commit()\n",
    "                print(each_code)\n",
    "                \n",
    "    with psycopg2.connect(**connect_param_local) as con:\n",
    "        with con.cursor() as cur:\n",
    "            # 한 컬럼이라도 NULL이 있는 ROW 삭제\n",
    "            query_delete_null_rows = sql.SQL(\"\"\"\n",
    "                delete from {table_name}\n",
    "                where not ({table_name} is not null);\n",
    "            \"\"\").format(table_name = sql.Identifier(table_name_argument))\n",
    "            cur.execute(query_delete_null_rows)\n",
    "            con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_finance_table_for_ml(prediction_range='6m', years=2, timedelta=730)\n",
    "create_finance_table_for_ml(prediction_range='6m', years=3, timedelta=1096)\n",
    "create_finance_table_for_ml(prediction_range='3m', years=2, timedelta=730)\n",
    "create_finance_table_for_ml(prediction_range='3m', years=3, timedelta=1096)\n",
    "create_finance_table_for_ml(prediction_range='1y', years=2, timedelta=730)\n",
    "create_finance_table_for_ml(prediction_range='1y', years=3, timedelta=1096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 뉴스 데이터 테이블 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 데이터 메인 테이블 만들기\n",
    "# 중복제거된 1224734개의 기사가 들어가게 됨\n",
    "with psycopg2.connect(**connect_param_local) as con:\n",
    "    with con.cursor() as cur:\n",
    "        cur.execute('drop table if exists news_data_deduplicated_main CASCADE')\n",
    "        con.commit()\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            CREATE TABLE news_data_deduplicated_main(\n",
    "                article_url text NOT NULL PRIMARY KEY\n",
    "                ,published_date date NOT NULL\n",
    "                ,article_title text\n",
    "                ,article_body text\n",
    "                ,bankruptcy_article_check boolean);\n",
    "            \"\"\"\n",
    "        )\n",
    "        con.commit()\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            COPY news_data_deduplicated_main\n",
    "            from '/usr/local/var/postgres/news_data_deduplicated_main.csv'\n",
    "            DELIMITER ','\n",
    "            CSV HEADER;\n",
    "            \"\"\"\n",
    "        )\n",
    "        con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 데이터 URL 테이블 만들기\n",
    "# 중복제거 되어있지 않음. 하나의 기사가 여러 기업에 연관되어있는 1:N의 관계이기 때문에, 이를 표현해 줄 이 테이블이 필요함\n",
    "# 총 2380889행\n",
    "with psycopg2.connect(**connect_param_local) as con:\n",
    "    with con.cursor() as cur:\n",
    "        cur.execute('drop table if exists news_data_total_url_for_tag')\n",
    "        con.commit()\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            CREATE TABLE news_data_total_url_for_tag(\n",
    "                stock_code char(6) NOT NULL\n",
    "                ,FOREIGN KEY (stock_code) REFERENCES target_company_list(stock_code)\n",
    "                ,company_name varchar(50) NOT NULL\n",
    "                ,article_url text NOT NULL\n",
    "                ,FOREIGN KEY (article_url) REFERENCES news_data_deduplicated_main(article_url));\n",
    "            \"\"\"\n",
    "        )\n",
    "        con.commit()\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            COPY news_data_total_url_for_tag\n",
    "            from '/usr/local/var/postgres/news_data_total_url_for_tag.csv'\n",
    "            DELIMITER ','\n",
    "            CSV HEADER;\n",
    "            \"\"\"\n",
    "        )\n",
    "        con.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
